{"container_type": "Author", "filled": ["basics", "publications", "indices", "counts"], "scholar_id": "j7AChUYAAAAJ", "source": "AUTHOR_PROFILE_PAGE", "name": "Huihong Shi", "url_picture": "https://scholar.googleusercontent.com/citations?view_op=view_photo&user=j7AChUYAAAAJ&citpid=1", "affiliation": "Nanjing University", "organization": 8752502527516415164, "interests": ["Efficient Machine Learning", "Algorithm Hardware Co-Design"], "email_domain": "@smail.nju.edu.cn", "homepage": "https://shihuihong214.github.io/huihong.shi/", "citedby": 176, "publications": {"j7AChUYAAAAJ:9yKSN-GCB0IC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Vitcod: Vision transformer acceleration via dedicated algorithm and accelerator co-design", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:9yKSN-GCB0IC", "num_citations": 55, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1009408517902769104", "cites_id": ["1009408517902769104"]}, "j7AChUYAAAAJ:u-x6o8ySG0sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Vitality: Unifying low-rank and sparse approximation for vision transformer acceleration with a linear taylor attention", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:u-x6o8ySG0sC", "num_citations": 36, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=11228380696103215227", "cites_id": ["11228380696103215227"]}, "j7AChUYAAAAJ:zYLM7Y9cAGgC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Instant-3d: Instant neural radiance field training towards on-device ar/vr 3d reconstruction", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:zYLM7Y9cAGgC", "num_citations": 27, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12798825553632228915", "cites_id": ["12798825553632228915"]}, "j7AChUYAAAAJ:IjCSPb-OGe4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ShiftAddNAS: Hardware-inspired search for more accurate and efficient neural networks", "pub_year": "2022"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:IjCSPb-OGe4C", "num_citations": 14, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17026416337828414455", "cites_id": ["17026416337828414455"]}, "j7AChUYAAAAJ:Tyk-4Ss8FVUC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ShiftAddViT: Mixture of multiplication primitives towards efficient vision transformer", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:Tyk-4Ss8FVUC", "num_citations": 12, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=4597100350747188343", "cites_id": ["4597100350747188343"]}, "j7AChUYAAAAJ:2osOgNQ5qMEC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Intelligent typography: Artistic text style transfer for complex texture and structure", "pub_year": "2022"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:2osOgNQ5qMEC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=9686193377382831291", "cites_id": ["9686193377382831291"]}, "j7AChUYAAAAJ:qjMakFHDy7sC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Max-affine spline insights into deep network pruning", "pub_year": "2021"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:qjMakFHDy7sC", "num_citations": 8, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=12609309598852783522,409514148922844850", "cites_id": ["12609309598852783522", "409514148922844850"]}, "j7AChUYAAAAJ:u5HHmVD_uO8C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NASA: Neural architecture search and acceleration for hardware inspired hybrid networks", "pub_year": "2022"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:u5HHmVD_uO8C", "num_citations": 5, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2102424561691043012", "cites_id": ["2102424561691043012"]}, "j7AChUYAAAAJ:d1gkVwhDpl0C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NASA+: Neural architecture search and acceleration for multiplication-reduced hybrid networks", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:d1gkVwhDpl0C", "num_citations": 4, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=7858301022394699375", "cites_id": ["7858301022394699375"]}, "j7AChUYAAAAJ:roLk4NBRz8UC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:roLk4NBRz8UC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2471563748126015039", "cites_id": ["2471563748126015039"]}, "j7AChUYAAAAJ:W7OEmFMy1HYC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NASA-F: FPGA-Oriented Search and Acceleration for Multiplication-Reduced Hybrid Networks", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:W7OEmFMy1HYC", "num_citations": 2, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=2045238044485301760", "cites_id": ["2045238044485301760"]}, "j7AChUYAAAAJ:LkGwnXOMwfcC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:LkGwnXOMwfcC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=6732000263319936629", "cites_id": ["6732000263319936629"]}, "j7AChUYAAAAJ:WF5omc3nYNoC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "An FPGA-Based Reconfigurable Accelerator for Convolution-Transformer Hybrid EfficientViT", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:WF5omc3nYNoC", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17884508973126581280", "cites_id": ["17884508973126581280"]}, "j7AChUYAAAAJ:YsMSGLbcyi4C": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "A computationally efficient neural video compression accelerator based on a sparse cnn-transformer hybrid network", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:YsMSGLbcyi4C", "num_citations": 1, "citedby_url": "https://scholar.google.com/scholar?oi=bibs&hl=en&cites=17929247382266615967", "cites_id": ["17929247382266615967"]}, "j7AChUYAAAAJ:Se3iqnhoufwC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "NASH: Neural Architecture and Accelerator Search for Multiplication-Reduced Hybrid Models", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:Se3iqnhoufwC", "num_citations": 0}, "j7AChUYAAAAJ:_FxGoFyzp5QC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "P-ViT: Power-of-Two Post-Training Quantization and Acceleration for Fully Quantized Vision Transformer", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:_FxGoFyzp5QC", "num_citations": 0}, "j7AChUYAAAAJ:ufrVoPGSRksC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "Trio-ViT: Post-Training Quantization and Acceleration for Softmax-Free Efficient Vision Transformer", "pub_year": "2024"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:ufrVoPGSRksC", "num_citations": 0}, "j7AChUYAAAAJ:Y0pCki6q_DkC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "SR: Exploring a Double-Win Transformer-Based Framework for Ideal and Blind Super-Resolution", "pub_year": "2023"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:Y0pCki6q_DkC", "num_citations": 0}, "j7AChUYAAAAJ:UeHWp8X0CEIC": {"container_type": "Publication", "source": "AUTHOR_PUBLICATION_ENTRY", "bib": {"title": "LITNet: A Light-weight Image Transform Net for Image Style Transfer", "pub_year": "2021"}, "filled": false, "author_pub_id": "j7AChUYAAAAJ:UeHWp8X0CEIC", "num_citations": 0}}, "citedby5y": 176, "hindex": 7, "hindex5y": 7, "i10index": 5, "i10index5y": 5, "cites_per_year": {"2021": 1, "2022": 6, "2023": 45, "2024": 122}, "updated": "2024-10-09 08:18:20.874497"}